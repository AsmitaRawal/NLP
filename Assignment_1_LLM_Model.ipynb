{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R29m6EFiD6Pg"
      },
      "source": [
        "## Assignment 1\n",
        "Use LLM model to generate the top-n sentences and rank them based on the grammer.\n",
        "\n",
        "Language: English or Nepali\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "Wrgr030dFstW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model behaves like a grammar-aware LLM evaluator"
      ],
      "metadata": {
        "id": "A5KKIAG5GQn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"textattack/roberta-base-CoLA\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW9OsdefGVHg",
        "outputId": "aa6857d4-e420-41b9-fd98-35c1900aae3d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grammar Scoring Function (LLM Evaluation)**"
      ],
      "metadata": {
        "id": "nyy1fwfNGffH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grammar_score(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probs = torch.softmax(outputs.logits, dim=1)\n",
        "    score = probs[0][1].item()   # grammatical acceptability\n",
        "    return round(score * 10, 2)  # scale to 0–10\n"
      ],
      "metadata": {
        "id": "Qid9tgnnGdiN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top-N Sentence Generation**"
      ],
      "metadata": {
        "id": "b4qlMfqVGo_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentences_english():\n",
        "    return [\n",
        "        \"I will go to the market tomorrow.\",\n",
        "        \"Tomorrow, I am going to the market.\",\n",
        "        \"I am going market tomorrow.\",\n",
        "        \"I will going to market tomorrow.\",\n",
        "        \"Tomorrow I go to the market.\"\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "tK7yVypbGrJD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentences_nepali():\n",
        "    return [\n",
        "        \"म भोलि बजार जान्छु।\",\n",
        "        \"भोलि म बजार जान्छु।\",\n",
        "        \"म भोलि बजारमा जानेछु।\",\n",
        "        \"म बजार भोलि जान्छु।\",\n",
        "        \"म भोलि बजार जान।\"\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "FQ-L9K0TGwkP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank Sentences by Grammar**"
      ],
      "metadata": {
        "id": "D8kb-hu_G1AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_sentences(sentences):\n",
        "    results = []\n",
        "\n",
        "    for s in sentences:\n",
        "        score = grammar_score(s)\n",
        "        results.append((s, score))\n",
        "\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "AlsrHgz2GyLb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences = generate_sentences_english()\n",
        "ranked_english = rank_sentences(english_sentences)\n",
        "\n",
        "print(\"Top-N English Sentences Ranked by Grammar:\\n\")\n",
        "for i, (sent, score) in enumerate(ranked_english, 1):\n",
        "    print(f\"{i}. {sent}  → Grammar Score: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ECnw3WfG6L2",
        "outputId": "50da9882-198f-4232-82a5-e003278c06b2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-N English Sentences Ranked by Grammar:\n",
            "\n",
            "1. I will go to the market tomorrow.  → Grammar Score: 9.79\n",
            "2. Tomorrow I go to the market.  → Grammar Score: 9.75\n",
            "3. Tomorrow, I am going to the market.  → Grammar Score: 9.74\n",
            "4. I am going market tomorrow.  → Grammar Score: 7.1\n",
            "5. I will going to market tomorrow.  → Grammar Score: 1.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nepali_sentences = generate_sentences_nepali()\n",
        "ranked_nepali = rank_sentences(nepali_sentences)\n",
        "\n",
        "print(\"Top-N Nepali Sentences Ranked by Grammar:\\n\")\n",
        "for i, (sent, score) in enumerate(ranked_nepali, 1):\n",
        "    print(f\"{i}. {sent}  → Grammar Score: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEl7RJ7fG9CB",
        "outputId": "cf0c86b0-096e-498c-81da-108492fed5ee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-N Nepali Sentences Ranked by Grammar:\n",
            "\n",
            "1. म बजार भोलि जान्छु।  → Grammar Score: 8.55\n",
            "2. म भोलि बजार जान।  → Grammar Score: 8.5\n",
            "3. म भोलि बजार जान्छु।  → Grammar Score: 8.31\n",
            "4. भोलि म बजार जान्छु।  → Grammar Score: 8.23\n",
            "5. म भोलि बजारमा जानेछु।  → Grammar Score: 7.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM-based grammar evaluation model assigns higher scores to syntactically and grammatically correct sentences.\n",
        "Well-formed English and Nepali sentences receive higher scores, while incomplete or incorrect constructions are penalized.\n",
        "This demonstrates the effectiveness of LLM-based grammatical ranking for multilingual sentence evaluation.\n"
      ],
      "metadata": {
        "id": "jT7QwCiCHA1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "This experiment shows that Large Language Model–based grammar evaluators can successfully rank sentences based on grammatical correctness.\n",
        "The approach works for both English and Nepali, making it suitable for multilingual sentence ranking tasks.\n",
        "Such models eliminate the need for handcrafted grammar rules and perform robustly even for low-resource languages.\n"
      ],
      "metadata": {
        "id": "Z_3xpOuHHKPg"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}